{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal # 정규분포 샘플링\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay buffer, Off-Policy: 다른 정책이 수집한 데이터를 기반으로 학습을 통해 정책 강화\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, maxlen=100000):\n",
    "        self.buffer = collections.deque(maxlen=maxlen)\n",
    "\n",
    "    def put(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, n=128):\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "\n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_prime, done_mask = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append(a)\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask_lst.append([done_mask])\n",
    "\n",
    "        return torch.tensor(np.array(s_lst), dtype=torch.float), \\\n",
    "               torch.tensor(np.array(a_lst), dtype=torch.float), \\\n",
    "               torch.tensor(np.array(r_lst), dtype=torch.float), \\\n",
    "               torch.tensor(np.array(s_prime_lst), dtype=torch.float), \\\n",
    "               torch.tensor(np.array(done_mask_lst), dtype=torch.float)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.std_min = -20\n",
    "        self.std_max = 2\n",
    "\n",
    "        # set the hidden layers\n",
    "        self.hidden1 = nn.Linear(3, 128)\n",
    "        self.hidden2 = nn.Linear(128, 128)\n",
    "\n",
    "        # set log_std layer\n",
    "        self.log_std_layer = nn.Linear(128, 1)\n",
    "\n",
    "        # set mean layer\n",
    "        self.mu_layer = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.hidden1(state))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "\n",
    "        # get mean\n",
    "        mu = self.mu_layer(x).tanh()\n",
    "\n",
    "        # get std\n",
    "        log_std = self.log_std_layer(x).tanh()\n",
    "        log_std = self.std_min + 0.5 * (self.std_max - self.std_min) * (log_std + 1)\n",
    "        std = torch.exp(log_std)\n",
    "\n",
    "        # sample actions\n",
    "        dist = Normal(mu, std)\n",
    "        z = dist.rsample()\n",
    "\n",
    "        # normalize action and log_prob\n",
    "        action = z.tanh()\n",
    "        log_prob = dist.log_prob(z) - torch.log(1 - action.pow(2) + 1e-7)\n",
    "        log_prob = log_prob.sum(-1, keepdim=True)\n",
    "\n",
    "        return action, log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CriticQ: 액션가치함수, 상태 s에서 액션을 선택하여 기대되는 Return 예측\n",
    "class CriticQ(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden1 = nn.Linear(3 + 1, 128)\n",
    "        self.hidden2 = nn.Linear(128, 128)\n",
    "        self.out = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat((state, action), dim=-1)\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        value = self.out(x)\n",
    "\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CriticV: 상태가치함수, 현재 상태가 얻을 Return 예측\n",
    "class CriticV(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden1 = nn.Linear(3, 128)\n",
    "        self.hidden2 = nn.Linear(128, 128)\n",
    "        self.out = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.hidden1(state))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        value = self.out(x)\n",
    "\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 액션 space가 [-1, 1] 사이가 아닌 경우 필요\n",
    "class ActionNormalizer(gym.ActionWrapper):\n",
    "    \"\"\"Rescale and relocate the actions.\"\"\"\n",
    "\n",
    "    def action(self, action: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Change the range (-1, 1) to (low, high).\"\"\"\n",
    "        low = self.action_space.low\n",
    "        high = self.action_space.high\n",
    "\n",
    "        scale_factor = (high - low) / 2\n",
    "        reloc_factor = high - scale_factor\n",
    "\n",
    "        action = action * scale_factor + reloc_factor\n",
    "        action = np.clip(action, low, high)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def reverse_action(self, action: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Change the range (low, high) to (-1, 1).\"\"\"\n",
    "        low = self.action_space.low\n",
    "        high = self.action_space.high\n",
    "\n",
    "        scale_factor = (high - low) / 2\n",
    "        reloc_factor = high - scale_factor\n",
    "\n",
    "        action = (action - reloc_factor) / scale_factor\n",
    "        action = np.clip(action, -1.0, 1.0)\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC:\n",
    "    def __init__(self):\n",
    "        self.tau = 5e-3\n",
    "\n",
    "        # automatic entropy tuning\n",
    "        self.target_entropy = -1\n",
    "        self.log_alpha = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "        # actor\n",
    "        self.actor = Actor()\n",
    "        # v function: 상태가치함수\n",
    "        self.critic_v = CriticV() # Behavior\n",
    "        self.critic_v_target = CriticV() # Target\n",
    "        self.critic_v_target.load_state_dict(self.critic_v.state_dict())\n",
    "        # q function: 액션가치함수\n",
    "        self.critic_q1 = CriticQ()\n",
    "        self.critic_q2 = CriticQ()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        selected_action = self.actor(torch.from_numpy(state).float())[0].detach().cpu().numpy()\n",
    "        return selected_action\n",
    "\n",
    "    def soft_update(self):\n",
    "        for t_param, l_param in zip(self.critic_v_target.parameters(), self.critic_v.parameters()):\n",
    "            t_param.data.copy_(self.tau * l_param.data + (1.0 - self.tau) * t_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sac = SAC()\n",
    "memory = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "OptimDic = collections.namedtuple('OptimDic', ['alpha_optim',\n",
    "                                               'actor_optim',\n",
    "                                               'critic_q1_optim',\n",
    "                                               'critic_q2_optim',\n",
    "                                               'critic_v_optim'])\n",
    "\n",
    "optim_dic = OptimDic(optim.Adam([sac.log_alpha], lr=3e-4),\n",
    "                     optim.Adam(sac.actor.parameters(), lr=3e-4),\n",
    "                     optim.Adam(sac.critic_q1.parameters(), lr=3e-4),\n",
    "                     optim.Adam(sac.critic_q2.parameters(), lr=3e-4),\n",
    "                     optim.Adam(sac.critic_v.parameters(), lr=3e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(sac, memory, optim_dic, gamma=0.99, train_actor=False):\n",
    "    state, action, reward, next_state, done_mask = memory.sample()\n",
    "    new_action, log_prob = sac.actor(state)\n",
    "\n",
    "    # Loss Function을 이용, 예측하는 값과의 차이 줄어들도록\n",
    "    # train alpha (dual problem)\n",
    "    alpha_loss = (-sac.log_alpha.exp() * (log_prob + sac.target_entropy).detach()).mean()\n",
    "    optim_dic.alpha_optim.zero_grad()\n",
    "    alpha_loss.backward()\n",
    "    optim_dic.alpha_optim.step()\n",
    "\n",
    "    alpha = sac.log_alpha.exp()  # used for the actor loss calculation\n",
    "\n",
    "    # q function loss\n",
    "    q1_pred = sac.critic_q1(state, action)\n",
    "    q2_pred = sac.critic_q2(state, action)\n",
    "    v_target = sac.critic_v_target(next_state)\n",
    "    q_target = reward + gamma * v_target * done_mask\n",
    "    q1_loss = F.mse_loss(q1_pred, q_target.detach())\n",
    "    q2_loss = F.mse_loss(q2_pred, q_target.detach())\n",
    "\n",
    "    # v function loss\n",
    "    v_pred = sac.critic_v(state)\n",
    "    q_pred = torch.min(sac.critic_q1(state, new_action), sac.critic_q2(state, new_action))\n",
    "    v_target = q_pred - alpha * log_prob\n",
    "    v_loss = F.mse_loss(v_pred, v_target.detach())\n",
    "\n",
    "    if train_actor:\n",
    "        # actor loss\n",
    "        advantage = q_pred - v_pred.detach()\n",
    "        actor_loss = (alpha * log_prob - advantage).mean()\n",
    "\n",
    "        # train actor\n",
    "        optim_dic.actor_optim.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        optim_dic.actor_optim.step()\n",
    "\n",
    "        # target update\n",
    "        sac.soft_update()\n",
    "\n",
    "    # train Q\n",
    "    optim_dic.critic_q1_optim.zero_grad()\n",
    "    q1_loss.backward()\n",
    "    optim_dic.critic_q1_optim.step()\n",
    "\n",
    "    optim_dic.critic_q2_optim.zero_grad()\n",
    "    q2_loss.backward()\n",
    "    optim_dic.critic_q2_optim.step()\n",
    "\n",
    "    # train V\n",
    "    optim_dic.critic_v_optim.zero_grad()\n",
    "    v_loss.backward()\n",
    "    optim_dic.critic_v_optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "ax: -0.16160, curr: -235.961543: 100%|█████████████████████████████████████████| 50000/50000 [03:17<00:00, 253.51it/s]"
     ]
    }
   ],
   "source": [
    "score = 0\n",
    "max_score = -99999999\n",
    "\n",
    "env = gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\")\n",
    "env = ActionNormalizer(env)\n",
    "s, _ = env.reset()\n",
    "\n",
    "p_bar = tqdm.trange(50000)\n",
    "for i in p_bar:\n",
    "    if i < 9999: # 일정 주기 랜덤 샘플\n",
    "        a = env.action_space.sample()\n",
    "    else:\n",
    "        a = sac.select_action(s)\n",
    "    s_prime, r, done, oob, _ = env.step(a)\n",
    "    done_mask = 0.0 if (done or oob) else 1.0\n",
    "    memory.put((s, a, r/100.0, s_prime, done_mask))\n",
    "\n",
    "    s = s_prime\n",
    "    score += r\n",
    "\n",
    "    if done or oob:\n",
    "        s, _ = env.reset()\n",
    "        p_bar.set_description(f'max: {max_score:.5f}, curr: {score:5f}')\n",
    "        if score >= max_score: # 성능 개선이 있었으면\n",
    "            torch.save(sac.actor.state_dict(), 'sac_actor.pth')\n",
    "            max_score = score\n",
    "        score = 0.0\n",
    "\n",
    "    if i >= 9999:\n",
    "        train(sac, memory, optim_dic, train_actor=(i % 3 == 0))\n",
    "\n",
    "p_bar.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sac = SAC()\n",
    "sac.actor.load_state_dict(torch.load('sac_actor.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-121.5875835976862\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\")\n",
    "env = ActionNormalizer(env)\n",
    "\n",
    "s, _ = env.reset()\n",
    "img = plt.imshow(env.render())\n",
    "\n",
    "score = 0.0\n",
    "while True:\n",
    "    # display\n",
    "    img.set_data(env.render())\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    a = sac.select_action(s)  # noise 없음\n",
    "    s_prime, r, done, oob, info = env.step(a)\n",
    "    s = s_prime\n",
    "    \n",
    "    score += r\n",
    "    if done or oob:\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
