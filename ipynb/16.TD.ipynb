{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0666f101-e743-4896-9c24-162c36c11687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a96b502-f08b-4e48-9aa7-4cbbc5534f06",
   "metadata": {},
   "source": [
    "# Temporal-Difference Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4641fcd8-dad5-4001-860d-738ca9f1d647",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld():\n",
    "    def __init__(self, n_row=4, n_col=4):\n",
    "        self.end_row = n_row - 1\n",
    "        self.end_col = n_col - 1\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "    \n",
    "    def step(self, a):\n",
    "        if a==0:\n",
    "            self.north()\n",
    "        elif a==1:\n",
    "            self.south()\n",
    "        elif a==2:\n",
    "            self.west()\n",
    "        elif a==3:\n",
    "            self.east()\n",
    "\n",
    "        reward = -1\n",
    "        done = (self.x == self.end_row and self.y == self.end_col)\n",
    "        return (self.x, self.y), reward, done\n",
    "\n",
    "    def north(self):\n",
    "        self.x = max(self.x - 1, 0)\n",
    "      \n",
    "    def south(self):\n",
    "        self.x = min(self.x + 1, self.end_row)\n",
    "      \n",
    "    def west(self):\n",
    "        self.y = max(self.y - 1, 0)\n",
    "  \n",
    "    def east(self):\n",
    "        self.y = min(self.y + 1, self.end_col)\n",
    "      \n",
    "    def reset(self):\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        return (self.x, self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b01ef315-3692-40e3-a32b-693c73c5c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDAgent():\n",
    "    def __init__(self):\n",
    "        self.alpha = 0.0001  # 업데이트 비율\n",
    "        self.v_table = np.zeros((4, 4))  # 상태 가치 테이블\n",
    "        self.pi = [0.25, 0.25, 0.25, 0.25]\n",
    "\n",
    "    def select_action(self, s):\n",
    "        action = np.random.choice(4, p=self.pi)\n",
    "        return action\n",
    "\n",
    "    def update_table(self, transition, gamma):\n",
    "        # 테이블의 값을 업데이트 한다\n",
    "        s, action, reward, s_prime = transition\n",
    "        x, y = s\n",
    "        x_prime, y_prime = s_prime\n",
    "        # TD 업데이트. v(s) ← (1 − α) * v(s) + α * (R + v(s′))\n",
    "        self.v_table[x, y] = (1 - self.alpha) * self.v_table[x, y] \\\n",
    "                             + self.alpha * (reward + gamma * self.v_table[x_prime, y_prime])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60758947-9565-4ece-9556-d2f732c01f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()  # 환경\n",
    "agent = TDAgent()  # 환경\n",
    "gamma = 1.0  # 환경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c0e62e1-c139-46ad-8870-0c49b5d789b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(50000):\n",
    "#for k in range(1):\n",
    "    done = False\n",
    "    # 에피소드 1회 진행\n",
    "    s = env.reset()\n",
    "    while not done:\n",
    "        action = agent.select_action(s)\n",
    "        s_prime, reward, done = env.step(action)\n",
    "        # 한 번의 step이 진행되자 마자 바로 테이블 업데이트, 몬테카를로방식과는 다름\n",
    "        agent.update_table((s, action, reward, s_prime), gamma)\n",
    "        # 상태 변경\n",
    "        s = s_prime\n",
    "    env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b93358c-38c0-431e-8301-9cd9e2c91fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-20.65, -19.49, -17.83, -16.57],\n",
       "       [-19.47, -18.16, -16.08, -14.3 ],\n",
       "       [-17.83, -16.11, -12.93,  -9.3 ],\n",
       "       [-16.57, -14.3 ,  -9.32,   0.  ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(agent.v_table, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d723fd-525d-4b45-85f9-1683111ba61f",
   "metadata": {},
   "source": [
    "# SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3af437a2-6925-440d-846d-06fe088e50e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld():\n",
    "    def __init__(self, n_row=7, n_col=7):\n",
    "        self.end_row = n_row - 1\n",
    "        self.end_col = n_col - 1\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        self.wall = {(0, 2), (1, 2), (2, 2), (3, 2), (4, 2), (6, 2),\n",
    "                     (0, 4), (2, 4), (3, 4), (4, 4), (5, 4), (6, 4)}\n",
    "    \n",
    "    def step(self, a):\n",
    "        if a==0:\n",
    "            self.north()\n",
    "        elif a==1:\n",
    "            self.south()\n",
    "        elif a==2:\n",
    "            self.west()\n",
    "        elif a==3:\n",
    "            self.east()\n",
    "\n",
    "        reward = -1\n",
    "        done = (self.x == self.end_row and self.y == self.end_col)\n",
    "        return (self.x, self.y), reward, done\n",
    "\n",
    "    def north(self):\n",
    "        if (max(self.x - 1, 0), self.y) not in self.wall:\n",
    "            self.x = max(self.x - 1, 0)\n",
    "      \n",
    "    def south(self):\n",
    "        if (min(self.x + 1, self.end_row), self.y) not in self.wall:\n",
    "            self.x = min(self.x + 1, self.end_row)\n",
    "      \n",
    "    def west(self):\n",
    "        if (self.x, max(self.y - 1, 0)) not in self.wall:\n",
    "            self.y = max(self.y - 1, 0)\n",
    "  \n",
    "    def east(self):\n",
    "        if (self.x, min(self.y + 1, self.end_col)) not in self.wall:\n",
    "            self.y = min(self.y + 1, self.end_col)\n",
    "      \n",
    "    def reset(self):\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        return (self.x, self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "494684a9-3354-4da4-a1aa-7cb65d30f228",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSAAgent():\n",
    "    def __init__(self):\n",
    "        self.q_table = np.zeros((7, 7, 4)) # q-table \n",
    "        self.alpha = 0.01  \n",
    "\n",
    "    def select_action(self, s, epsilon=0.0): # E-greedy\n",
    "        x, y = s\n",
    "        dice = np.random.random()\n",
    "        if dice < epsilon:\n",
    "            action = np.random.randint(0, 4)\n",
    "        else:\n",
    "            action_val = self.q_table[x,y,:]\n",
    "            #print(action_val, \" is \", np.argmax(action_val))\n",
    "            action = np.argmax(action_val)\n",
    "        return action\n",
    "        \n",
    "    def update_table(self, transition, epsilon):\n",
    "        s, a, r, s_prime = transition\n",
    "        x,y = s\n",
    "        next_x, next_y = s_prime\n",
    "        next_a = self.select_action(s_prime, epsilon) # S'에서 선택할 액션 (실제로 취한 액션이 아님), E-greedy\n",
    "        # SARSA 업데이트.\n",
    "        self.q_table[x,y,a] = (1 - self.alpha) * self.q_table[x,y,a] \\\n",
    "                              + self.alpha * (r + self.q_table[next_x,next_y,next_a])\n",
    "\n",
    "    def show_table(self):\n",
    "        q_lst = self.q_table.tolist()\n",
    "        data = np.zeros((7, 7))\n",
    "        for row_idx in range(len(q_lst)):\n",
    "            row = q_lst[row_idx]\n",
    "            for col_idx in range(len(row)):\n",
    "                col = row[col_idx]\n",
    "                action = np.argmax(col) if min(col) != 0 else -1\n",
    "                data[row_idx, col_idx] = action\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60af8dde-647e-4c55-b3ea-896fd296c77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()  # 환경\n",
    "agent = SARSAAgent()  # 에이전트\n",
    "epsilon = 0.9  # 탐험 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6296ae84-5ca4-4ce8-a7e8-740f75390f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(50000):\n",
    "    done = False\n",
    "    # env 초기화\n",
    "    s = env.reset()\n",
    "    # 에피소드 1회 진행\n",
    "    while not done:\n",
    "        a = agent.select_action(s, epsilon)\n",
    "        s_prime, r, done = env.step(a)\n",
    "        agent.update_table((s, a, r, s_prime), epsilon)  # 매번 업데이트\n",
    "        s = s_prime\n",
    "    # decaying epsilon\n",
    "    epsilon = max(epsilon - 0.001, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a9fa663-31f0-41a8-901a-92bd1c12709e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1., -1.,  1., -1.,  1.,  1.],\n",
       "       [ 1.,  1., -1.,  3.,  3.,  1.,  1.],\n",
       "       [ 1.,  1., -1.,  0., -1.,  1.,  1.],\n",
       "       [ 3.,  1., -1.,  0., -1.,  1.,  1.],\n",
       "       [ 1.,  1., -1.,  0., -1.,  1.,  1.],\n",
       "       [ 3.,  3.,  3.,  0., -1.,  1.,  1.],\n",
       "       [ 3.,  0., -1.,  0., -1.,  3., -1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.show_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07fece0-1ab9-4de3-a2cf-1d15ea28e543",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcd8b46e-54c4-44f4-8962-d38216abffa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld():\n",
    "    def __init__(self, n_row=7, n_col=7):\n",
    "        self.end_row = n_row - 1\n",
    "        self.end_col = n_col - 1\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        self.wall = {(0, 2), (1, 2), (2, 2), (3, 2), (4, 2), (6, 2),\n",
    "                     (0, 4), (2, 4), (3, 4), (4, 4), (5, 4), (6, 4)}\n",
    "    \n",
    "    def step(self, a):\n",
    "        if a==0:\n",
    "            self.north()\n",
    "        elif a==1:\n",
    "            self.south()\n",
    "        elif a==2:\n",
    "            self.west()\n",
    "        elif a==3:\n",
    "            self.east()\n",
    "\n",
    "        reward = -1\n",
    "        done = (self.x == self.end_row and self.y == self.end_col)\n",
    "        return (self.x, self.y), reward, done\n",
    "\n",
    "    def north(self):\n",
    "        if (max(self.x - 1, 0), self.y) not in self.wall:\n",
    "            self.x = max(self.x - 1, 0)\n",
    "      \n",
    "    def south(self):\n",
    "        if (min(self.x + 1, self.end_row), self.y) not in self.wall:\n",
    "            self.x = min(self.x + 1, self.end_row)\n",
    "      \n",
    "    def west(self):\n",
    "        if (self.x, max(self.y - 1, 0)) not in self.wall:\n",
    "            self.y = max(self.y - 1, 0)\n",
    "  \n",
    "    def east(self):\n",
    "        if (self.x, min(self.y + 1, self.end_col)) not in self.wall:\n",
    "            self.y = min(self.y + 1, self.end_col)\n",
    "      \n",
    "    def reset(self):\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        return (self.x, self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c22b59c3-5edf-4b9d-8dde-d9e3e987eb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent():\n",
    "    def __init__(self):\n",
    "        self.q_table = np.zeros((7, 7, 4)) # q-table \n",
    "        self.alpha = 0.01  \n",
    "\n",
    "    def select_action(self, s, epsilon=0.0): # E-greedy\n",
    "        x, y = s\n",
    "        dice = np.random.random()\n",
    "        if dice < epsilon:\n",
    "            action = np.random.randint(0, 4)\n",
    "        else:\n",
    "            action_val = self.q_table[x, y, :]\n",
    "            action = np.argmax(action_val)\n",
    "        return action\n",
    "        \n",
    "    def update_table(self, transition):\n",
    "        s, a, r, s_prime = transition  # behavior 행동 결과\n",
    "        x, y = s\n",
    "        x_prime, y_prime = s_prime\n",
    "        a_prime = np.argmax(self.q_table[x_prime, y_prime, :])  # target 정책, Greedy\n",
    "        # Q-Learning 업데이트.\n",
    "        self.q_table[x, y, a] = (1 - self.alpha) * self.q_table[x, y, a] \\\n",
    "                                + self.alpha * (r + self.q_table[x_prime, y_prime, a_prime])\n",
    "\n",
    "    def show_table(self):\n",
    "        q_lst = self.q_table.tolist()\n",
    "        data = np.zeros((7, 7))\n",
    "        for row_idx in range(len(q_lst)):\n",
    "            row = q_lst[row_idx]\n",
    "            for col_idx in range(len(row)):\n",
    "                col = row[col_idx]\n",
    "                action = np.argmax(col) if min(col) != 0 else -1\n",
    "                data[row_idx, col_idx] = action\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "023572cc-ea0b-420f-aa63-ce2eb924a6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()  # 환경\n",
    "agent = QAgent()  # 에이전트\n",
    "epsilon = 0.9  # 탐험 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "800cd272-59f9-4ab6-b338-fbdc45e598e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(50000):\n",
    "    done = False\n",
    "    # env 초기화\n",
    "    s = env.reset()\n",
    "    # 에피소드 1회 진행\n",
    "    while not done:\n",
    "        a = agent.select_action(s, epsilon)\n",
    "        s_prime, r, done = env.step(a)\n",
    "        agent.update_table((s, a, r, s_prime))  # 매번 업데이트\n",
    "        s = s_prime\n",
    "    # decaying epsilon\n",
    "    epsilon = max(epsilon - 0.001, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7a07ee6-738c-4f98-8da1-aea1b6117ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1., -1.,  1., -1.,  1.,  1.],\n",
       "       [ 1.,  1., -1.,  3.,  3.,  1.,  1.],\n",
       "       [ 1.,  1., -1.,  0., -1.,  1.,  1.],\n",
       "       [ 1.,  1., -1.,  0., -1.,  1.,  1.],\n",
       "       [ 1.,  1., -1.,  0., -1.,  1.,  1.],\n",
       "       [ 3.,  3.,  3.,  0., -1.,  1.,  1.],\n",
       "       [ 0.,  0., -1.,  0., -1.,  3., -1.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.show_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c2d2e2-6c4b-4d98-8456-a4efcad8bf98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
