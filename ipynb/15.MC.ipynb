{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte-Carlo Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld():\n",
    "    def __init__(self, n_row=4, n_col=4):\n",
    "        self.end_row = n_row - 1\n",
    "        self.end_col = n_col - 1\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "    \n",
    "    def step(self, a):\n",
    "        if a==0:\n",
    "            self.north()\n",
    "        elif a==1:\n",
    "            self.south()\n",
    "        elif a==2:\n",
    "            self.west()\n",
    "        elif a==3:\n",
    "            self.east()\n",
    "\n",
    "        reward = -1\n",
    "        done = (self.x == self.end_row and self.y == self.end_col)\n",
    "        return (self.x, self.y), reward, done\n",
    "\n",
    "    def north(self):\n",
    "        self.x = max(self.x - 1, 0)\n",
    "      \n",
    "    def south(self):\n",
    "        self.x = min(self.x + 1, self.end_row)\n",
    "      \n",
    "    def west(self):\n",
    "        self.y = max(self.y - 1, 0)\n",
    "  \n",
    "    def east(self):\n",
    "        self.y = min(self.y + 1, self.end_col)\n",
    "      \n",
    "    def reset(self):\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        return (self.x, self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self):\n",
    "        self.alpha = 0.0001  # 업데이트 비율\n",
    "        #self.alpha = 1.0  # 업데이트 비율\n",
    "        self.v_table = np.zeros((4, 4))  # 상태 가치 테이블\n",
    "        self.pi = [0.25, 0.25, 0.25, 0.25]\n",
    "\n",
    "    def select_action(self, s):\n",
    "        action = np.random.choice(4, p=self.pi)\n",
    "        return action\n",
    "\n",
    "    def update_table(self, history, gamma):\n",
    "        # 테이블의 값을 업데이트 한다\n",
    "        cum_reward = 0\n",
    "        for transition in history[::-1]:  # history의 뒤쪽부터 차례로 리턴 계산\n",
    "            s, action, reward, s_prime = transition\n",
    "            x, y = s\n",
    "            # 몬테 카를로 업데이트. v(s) ← (1 − α) * v(s) + α * G\n",
    "            self.v_table[x, y] = (1 - self.alpha) * self.v_table[x, y] + self.alpha * cum_reward\n",
    "            cum_reward = reward + gamma * cum_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()  # 환경\n",
    "agent = Agent()  # 에이전트\n",
    "gamma = 1.0  # 감쇄인자, 미래에 받을 보상이라도 현재와 동일한 가중치를 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(50000):\n",
    "    done = False\n",
    "    history = []\n",
    "    # 에피소드 1회 진행\n",
    "    s = env.reset()\n",
    "    while not done:\n",
    "        action = agent.select_action(s)\n",
    "        s_prime, reward, done = env.step(action)\n",
    "        history.append((s, action, reward, s_prime))\n",
    "        # 상태 변경\n",
    "        s = s_prime\n",
    "\n",
    "    agent.update_table(history, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-58.46, -56.45, -52.76, -51.56],\n",
       "       [-56.51, -53.64, -47.96, -44.45],\n",
       "       [-53.11, -48.85, -39.73, -29.3 ],\n",
       "       [-51.08, -43.97, -28.78,   0.  ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(agent.v_table, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld():\n",
    "    def __init__(self, n_row=7, n_col=7):\n",
    "        self.end_row = n_row - 1\n",
    "        self.end_col = n_col - 1\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        self.wall = {(0, 2), (1, 2), (2, 2), (3, 2), (4, 2), (6, 2),\n",
    "                     (0, 4), (2, 4), (3, 4), (4, 4), (5, 4), (6, 4)}\n",
    "    \n",
    "    def step(self, a):\n",
    "        if a==0:\n",
    "            self.north()\n",
    "        elif a==1:\n",
    "            self.south()\n",
    "        elif a==2:\n",
    "            self.west()\n",
    "        elif a==3:\n",
    "            self.east()\n",
    "\n",
    "        reward = -1\n",
    "        done = (self.x == self.end_row and self.y == self.end_col)\n",
    "        return (self.x, self.y), reward, done\n",
    "\n",
    "    def north(self):\n",
    "        if (max(self.x - 1, 0), self.y) not in self.wall:\n",
    "            self.x = max(self.x - 1, 0)\n",
    "      \n",
    "    def south(self):\n",
    "        if (min(self.x + 1, self.end_row), self.y) not in self.wall:\n",
    "            self.x = min(self.x + 1, self.end_row)\n",
    "      \n",
    "    def west(self):\n",
    "        if (self.x, max(self.y - 1, 0)) not in self.wall:\n",
    "            self.y = max(self.y - 1, 0)\n",
    "  \n",
    "    def east(self):\n",
    "        if (self.x, min(self.y + 1, self.end_col)) not in self.wall:\n",
    "            self.y = min(self.y + 1, self.end_col)\n",
    "      \n",
    "    def reset(self):\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        return (self.x, self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCAgent():\n",
    "    def __init__(self):\n",
    "        self.q_table = np.zeros((7, 7, 4)) # q-table \n",
    "        self.alpha = 0.01\n",
    "\n",
    "    def select_action(self, s, epsilon=0.0):\n",
    "        x, y = s\n",
    "        dice = np.random.random()\n",
    "        if dice < epsilon:\n",
    "            action = np.random.randint(0, 4) # 랜덤 선택\n",
    "        else:\n",
    "            action_val = self.q_table[x,y,:] # 가장 확률이 높은 경우 선택\n",
    "            action = np.argmax(action_val)\n",
    "        return action\n",
    "        \n",
    "    def update_table(self, history, gamma):\n",
    "        # q 테이블의 값을 업데이트 한다\n",
    "        cum_reward = 0\n",
    "        for transition in history[::-1]:\n",
    "            s, a, r, s_prime = transition\n",
    "            x,y = s\n",
    "            # 몬테 카를로 업데이트. q(s, a) ← (1 − α) * q(s, a) + α * G\n",
    "            self.q_table[x, y, a] = (1 - self.alpha) * self.q_table[x, y, a] + self.alpha * cum_reward\n",
    "            cum_reward = r + gamma * cum_reward\n",
    "\n",
    "    def show_table(self):\n",
    "        q_lst = self.q_table.tolist()\n",
    "        data = np.zeros((7, 7))\n",
    "        for row_idx in range(len(q_lst)):\n",
    "            row = q_lst[row_idx]\n",
    "            for col_idx in range(len(row)):\n",
    "                col = row[col_idx]\n",
    "                action = np.argmax(col) if min(col) != 0 else -1\n",
    "                data[row_idx, col_idx] = action\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()  # 환경\n",
    "agent = MCAgent()  # 에이전트\n",
    "gamma = 1.0  # 감쇄인자\n",
    "epsilon = 0.9  # 탐험 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(50000):\n",
    "    done = False\n",
    "    history = []\n",
    "    # env 초기화\n",
    "    s = env.reset()\n",
    "    # 에피소드 1회 진행\n",
    "    while not done:\n",
    "        a = agent.select_action(s, epsilon)\n",
    "        s_prime, r, done = env.step(a)\n",
    "        history.append((s, a, r, s_prime))\n",
    "        s = s_prime\n",
    "    # 에피소드 종료 후 테이블 업데이트\n",
    "    agent.update_table(history, gamma)\n",
    "    # decaying epsilon\n",
    "    epsilon = max(epsilon - 0.001, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2., -1.,  1., -1.,  3.,  1.],\n",
       "       [ 1.,  2., -1.,  3.,  3.,  1.,  1.],\n",
       "       [ 1.,  1., -1.,  0., -1.,  1.,  1.],\n",
       "       [ 1.,  1., -1.,  0., -1.,  1.,  1.],\n",
       "       [ 3.,  1., -1.,  0., -1.,  1.,  1.],\n",
       "       [ 3.,  3.,  3.,  0., -1.,  1.,  1.],\n",
       "       [ 0.,  0., -1.,  0., -1.,  3., -1.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.show_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
